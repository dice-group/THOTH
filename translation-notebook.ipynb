{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QALD8 questions\n",
    "# Extract entities (with German sameAs) out of SPARQL in QALD8\n",
    "# Extract German and English subgraph using entities in QALD8\n",
    "# The steps below apply only to the German Subgraph\n",
    "# Extract all the triples with string literals into a file (string-triples.nt)\n",
    "# Writes two files:\n",
    "    # 1. Triples with replaced string literal (<UNK>) (sub-str-triples.tsv)\n",
    "    # 2. String literals corresponding to triples in file1 (str-literals.txt)\n",
    "# Extract all non-literal triples into a file (non-literal-triples.tsv)\n",
    "# Preprocess the triples and keep a map between preprocessed token and original uri\n",
    "# Perform the triple translation\n",
    "# Perform the text translation using KG-NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qald-7-train-multilingual\n"
     ]
    }
   ],
   "source": [
    "# Load QALD8 questions\n",
    "\n",
    "# import urllib library\n",
    "from urllib.request import urlopen\n",
    "  \n",
    "# import json\n",
    "import json\n",
    "# store the URL in url as \n",
    "# parameter for urlopen\n",
    "url = \"https://raw.githubusercontent.com/ag-sc/QALD/master/7/data/qald-7-train-multilingual.json\"\n",
    "  \n",
    "# store the response of URL\n",
    "response = urlopen(url)\n",
    "  \n",
    "# storing the JSON response \n",
    "# from url in data\n",
    "data_json = json.loads(response.read())\n",
    "  \n",
    "# print the json response\n",
    "print(data_json['dataset']['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities found:  213\n",
      "Number of spaced strings:  214\n"
     ]
    }
   ],
   "source": [
    "# Extract entities (with German sameAs) out of SPARQL in QALD8\n",
    "\n",
    "import re\n",
    "ent_list = set({})\n",
    "spc_sep_ents = ''\n",
    "sep_ents_arr = []\n",
    "limit = 1\n",
    "count = 0\n",
    "for question in data_json['questions']:\n",
    "    sparql = question['query']['sparql']\n",
    "    temp_res1 = re.findall('res:[^\\s\\.]+',sparql)\n",
    "    temp_res1 = [w.replace('res:', 'http://dbpedia.org/resource/') for w in temp_res1]\n",
    "    temp_res2 = re.findall('http://dbpedia.org/resource/[^>]+', sparql)\n",
    "    ent_list.update(temp_res1)\n",
    "    ent_list.update(temp_res2)\n",
    "    # print(temp_res1, temp_res2)\n",
    "\n",
    "# print(ent_list)\n",
    "    \n",
    "spc_sep_ents = ''\n",
    "for ent in ent_list:\n",
    "    count+=1\n",
    "    spc_sep_ents+='<'+ent+'> '\n",
    "    if count%limit == 0 :\n",
    "        sep_ents_arr.append(spc_sep_ents)\n",
    "        spc_sep_ents = ''\n",
    "sep_ents_arr.append(spc_sep_ents)\n",
    "\n",
    "print('Number of entities found: ', len(ent_list))\n",
    "print('Number of spaced strings: ', len(sep_ents_arr))\n",
    "\n",
    "# print(sep_ents_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 2\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 1\n",
      "Result size: 0\n",
      "Result size: 0\n",
      "Total number of English entries:  190\n",
      "Total number of German entries:  209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "en_ents = set({})\n",
    "de_ents = set({})\n",
    "\n",
    "for sep_ents_str in sep_ents_arr :\n",
    "    sameas_query = \"\"\"SELECT ?ent ?ent_de WHERE {\n",
    "    ?ent owl:sameAs ?ent_de . \n",
    "    FILTER(regex(str(?ent_de ), \"http://de.dbpedia.org/resource/\" )) \"\"\"\n",
    "    sameas_query+='VALUES ?ent { '+ sep_ents_str +'}} LIMIT 2'\n",
    "\n",
    "    # print('query: ', sameas_query)\n",
    "    \n",
    "    sparql.setQuery(sameas_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    print('Result size:',len(results[\"results\"][\"bindings\"]))\n",
    "\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        en_ents.add(result['ent']['value'])\n",
    "        de_ents.add(result['ent_de']['value'])\n",
    "        \n",
    "    time.sleep(0.1)\n",
    "    \n",
    "print('Total number of English entries: ',len(en_ents))\n",
    "print('Total number of German entries: ',len(de_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query response size:  140657\n",
      "query response size:  77206\n",
      "query response size:  307032\n",
      "query response size:  199786\n",
      "query response size:  48406\n",
      "query response size:  266263\n",
      "query response size:  34515\n",
      "query response size:  356715\n",
      "query response size:  114366\n",
      "query response size:  195341\n",
      "query response size:  56020\n",
      "query response size:  68024\n",
      "query response size:  111657\n",
      "query response size:  155502\n",
      "query response size:  257083\n",
      "query response size:  217448\n",
      "query response size:  512006\n",
      "query response size:  516853\n",
      "query response size:  787610\n",
      "Size of English Sub-Graph:  4421499\n"
     ]
    }
   ],
   "source": [
    "# Extract German and English subgraph using entities in QALD8\n",
    "\n",
    "# English Sub-Graph\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sep_ents_arr = []\n",
    "limit = 10\n",
    "count = 0\n",
    "spc_sep_ents = ''\n",
    "\n",
    "for ent in en_ents:\n",
    "    count+=1\n",
    "    spc_sep_ents+='<'+ent+'> '\n",
    "    if count%limit == 0 :\n",
    "        sep_ents_arr.append(spc_sep_ents)\n",
    "        spc_sep_ents = ''\n",
    "if len(spc_sep_ents) > 0:\n",
    "    sep_ents_arr.append(spc_sep_ents)\n",
    "\n",
    "en_subgraph = Graph()\n",
    "\n",
    "for sep_ents_str in sep_ents_arr :\n",
    "    describe_query = 'DESCRIBE ?ent WHERE { VALUES ?ent { '+ sep_ents_str +' } }'\n",
    "    sparql.setQuery(describe_query)\n",
    "    sparql.setReturnFormat('rdf')\n",
    "    results = sparql.query().convert()\n",
    "    print('query response size: ',len(results))\n",
    "    en_subgraph+=results\n",
    "    time.sleep(1)\n",
    "print('Size of English Sub-Graph: ', len(en_subgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query response size:  30419\n",
      "query response size:  4602\n",
      "query response size:  78518\n",
      "query response size:  2014\n",
      "query response size:  780\n",
      "query response size:  898\n",
      "query response size:  11396\n",
      "query response size:  10187\n",
      "query response size:  959\n",
      "query response size:  8600\n",
      "query response size:  29905\n",
      "query response size:  1494\n",
      "query response size:  8726\n",
      "query response size:  2234\n",
      "query response size:  5901\n",
      "query response size:  4361\n",
      "query response size:  891\n",
      "query response size:  15245\n",
      "query response size:  22581\n",
      "query response size:  1069\n",
      "query response size:  2286\n",
      "Size of German Sub-Graph:  243001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# German Sub-Graph\n",
    "sparql = SPARQLWrapper(\"http://de.dbpedia.org/sparql\")\n",
    "sep_ents_arr = []\n",
    "limit = 10\n",
    "count = 0\n",
    "spc_sep_ents = ''\n",
    "\n",
    "for ent in de_ents:\n",
    "    count+=1\n",
    "    spc_sep_ents+='<'+ent+'> '\n",
    "    if count%limit == 0 :\n",
    "        sep_ents_arr.append(spc_sep_ents)\n",
    "        spc_sep_ents = ''\n",
    "if len(spc_sep_ents) > 0:\n",
    "    sep_ents_arr.append(spc_sep_ents)\n",
    "\n",
    "de_subgraph = Graph()\n",
    "for sep_ents_str in sep_ents_arr :\n",
    "    describe_query = 'DESCRIBE ?ent WHERE { VALUES ?ent { '+ sep_ents_str +' } }'\n",
    "    sparql.setQuery(describe_query)\n",
    "    sparql.setReturnFormat('rdf')\n",
    "    results = sparql.query().convert()\n",
    "    print('query response size: ', len(results))\n",
    "    de_subgraph+= results\n",
    "    time.sleep(1)\n",
    "print('Size of German Sub-Graph: ', len(de_subgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write Sub-Graphs to a file\n",
    "en_subgraph.serialize(destination=\"lf-translation/en-sub-graph.nt\",format='nt')\n",
    "de_subgraph.serialize(destination=\"lf-translation/de-sub-graph.nt\",format='nt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below can be used directly if the sub-graphs already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English Sub-Graph:  4421499\n",
      "Size of German Sub-Graph:  243001\n"
     ]
    }
   ],
   "source": [
    "# Load the sub-graph\n",
    "from rdflib import Graph\n",
    "\n",
    "en_subgraph = Graph()\n",
    "en_subgraph.parse(source=\"lf-translation/en-sub-graph.nt\", format='nt')\n",
    "\n",
    "de_subgraph = Graph()\n",
    "de_subgraph.parse(source=\"lf-translation/de-sub-graph.nt\", format='nt')\n",
    "\n",
    "print('Size of English Sub-Graph: ', len(en_subgraph))\n",
    "print('Size of German Sub-Graph: ', len(de_subgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The steps below apply only to the German Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI Preprocessing function\n",
    "\n",
    "# from rdflib import Graph, URIRef\n",
    "# nsg = Graph()\n",
    "# nsg.bind(\"dbo\", 'http://dbpedia.org/ontology/')\n",
    "# nsg.bind(\"dbr_en\", 'http://dbpedia.org/resource/')\n",
    "# nsg.bind(\"dbp\", 'http://dbpedia.org/property/')\n",
    "# nsg.bind(\"dbr_de\", 'http://de.dbpedia.org/resource/')\n",
    "# nsg.bind(\"dbp_de\", 'http://de.dbpedia.org/property/')\n",
    "# # all_ns = [n for n in g.namespace_manager.namespaces()]\n",
    "# # print(all_ns)\n",
    "# uri_map = {}\n",
    "# def process_uri(uri_ref):\n",
    "#     # Check map if URI exists; if yes, then return corresponding token. Else:\n",
    "#     uri_str = str(uri_ref)\n",
    "#     if uri_str in uri_map:\n",
    "#         return uri_map[str(uri_ref)]\n",
    "#     # replace dbo, dbp, dbr namespace with prefixes & for new URIs, generate new prefix\n",
    "#     frag = nsg.compute_qname(uri_ref)\n",
    "#     retval = frag[0] + '_' + frag[2]\n",
    "#     retval = retval.lower()\n",
    "#     # Map the end result to its URI\n",
    "#     uri_map[str(uri_ref)] = retval\n",
    "#     return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "uri_map = {}\n",
    "# uri_map = json.load(open(\"lf-translation/uri-map\"))\n",
    "def clean(uri_ref):\n",
    "    # Check map if URI exists; if yes, then return corresponding token. Else:\n",
    "    uri_str = str(uri_ref)\n",
    "    if uri_str in uri_map:\n",
    "        return uri_map[str(uri_ref)]\n",
    "    string = uri_str.replace(\"http://dbpedia.org/ontology/\", \"dbo_\")\n",
    "    string = string.replace(\"http://dbpedia.org/property/\", \"dbp_\")\n",
    "    string = string.replace(\"http://dbpedia.org/resource/\", \"dbr_en_\")\n",
    "    string = string.replace(\"http://de.dbpedia.org/property/\", \"dbp_de_\")\n",
    "    string = string.replace(\"http://de.dbpedia.org/resource/\", \"dbr_de_\")\n",
    "    string = re.sub(r'\\W+', '', string)\n",
    "    string = string.lower()\n",
    "    # Map the end result to its URI\n",
    "    uri_map[str(uri_ref)] = string\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed literal triples ready.\n"
     ]
    }
   ],
   "source": [
    "# Extract all the triples with string literals into a file (string-triples.tsv)\n",
    "# TODO: \n",
    "string_sparql = 'SELECT ?s ?p ?o WHERE {?s ?p ?o . FILTER ( datatype(?o) = <http://www.w3.org/1999/02/22-rdf-syntax-ns#langString> && lang(?o) = \"de\") } '\n",
    "qres = de_subgraph.query(string_sparql)\n",
    "# Writes two files:\n",
    "    # 1. Triples with replaced string literal (<UNK>) (sub-str-triples.tsv)\n",
    "    # 2. String literals corresponding to triples in file1 (str-literals.txt)\n",
    "with open('lf-translation/de-str-triples.tsv', 'w') as str_out, \\\n",
    "    open('lf-translation/de-str-rep-triples-preprocessed.txt', 'w') as str_rep_out, \\\n",
    "    open('lf-translation/de-str-only.txt', 'w') as only_str_out:\n",
    "    for row in qres:\n",
    "        # string triples out\n",
    "        str_out.write(str(row[0]) + '\\t' + str(row[1]) + '\\t\"' + str(row[2]) + '\"\\n')\n",
    "        # replaced/preprocessed triples\n",
    "        str_rep_out.write(clean(row[0]) + ' ' + clean(row[1]) + ' <UNK>\\n')\n",
    "        # only literals\n",
    "        only_str_out.write(str(row[2]) + '\\n')\n",
    "print('Pre-processed literal triples ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed non-literal triples ready.\n"
     ]
    }
   ],
   "source": [
    "# Extract all non-literal triples into a file (non-literal-triples.tsv)\n",
    "nolit_sparql = 'SELECT ?s ?p ?o WHERE {?s ?p ?o . FILTER(!isLiteral(?o) ) }'\n",
    "qres = de_subgraph.query(nolit_sparql)\n",
    "with open('lf-translation/de-non-literal-triples.tsv', 'w') as nolit_out, \\\n",
    "     open('lf-translation/de-non-literal-triples-preprocessed.txt', 'w') as nolit_pp_out   :\n",
    "     for row in qres:\n",
    "        # triples out\n",
    "        nolit_out.write(str(row[0]) + '\\t' + str(row[1]) + '\\t' + str(row[2]) + '\\n')\n",
    "        # preprocessed triples out\n",
    "        nolit_pp_out.write(clean(row[0]) + ' ' + clean(row[1]) + ' ' + clean(row[2]) + '\\n')\n",
    "print('Pre-processed non-literal triples ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI Map written to file.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the triples and keep a map between preprocessed token and original uri\n",
    "# ^^ This step is done along with the extraction above.\n",
    "# Save the Unique URI Map\n",
    "import json\n",
    "with open('lf-translation/uri-map', 'w') as out:\n",
    "    out.write(json.dumps(uri_map, indent=4, sort_keys=True))\n",
    "print('URI Map written to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the triple translation (Must be done in THOTH's python environment)\n",
    "# python -m nmt.nmt  --vocab_prefix=../$1/vocab --model_dir=../$1_model  --inference_input_file=./to_ask.txt  --inference_output_file=./output.txt --out_dir=../$1_model --src=en --tgt=sparql > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the text translation using KG-NMT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
